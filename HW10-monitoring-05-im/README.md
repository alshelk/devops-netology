# Домашнее задание к занятию 17 «Инцидент-менеджмент»

## Задание

Составьте постмортем на основе реального сбоя системы GitHub в 2018 году.

Информацию о сбое можно изучить по ссылкам ниже:

* [краткое описание на русском языке](https://habr.com/ru/post/427301/);
* [развёрнутое описание на английском языке](https://github.blog/2018-10-30-oct21-post-incident-analysis/).


<details>
<summary>

### Постмортем:

</summary>


#### Краткое описание инцидента:

 В 22:52 (21.10) по UTC на нескольких сервисах GitHub.com пострадали несколько сетевых разделов и последующим сбоем 
 базы данных, что привело к появлению непоследовательной информации на нашем веб-сайте. 

#### Плановые работы по замене оптического оборудования:

Плановые работы по замене оптического оборудования

#### Причины инцидента:

Плановые работы по техническому обслуживанию по замене вышедшего из строя оптического оборудования 100G привели к 
потере соединения между нашим сетевым концентратором на восточном побережье США и нашим основным центром обработки 
данных на восточном побережье США.

#### Воздействие: 

Соединение было восстановлено за 43 секунды, но этот кратковременный сбой вызвал цепочку событий, которые привели к 
ухудшению качества обслуживания на 24 часа и 11 минут 

#### Обнаружение:

в 22:54 системы мониторинга начали генерировать оповещения и дежурные инженеры обнаружили проблему и к 23:02 UTC 
инженеры нашей группы быстрого реагирования определили, что топологии многочисленных кластеров баз данных находятся в 
неожиданном состоянии.

#### Реакция:

Обслуживание было восстановлено за 24 часа и 11 минут

#### Восстановление:

Данные были восстановлены из резервных копий, синхронизированы реплики на обоих сайтах, после чего вернулись к 
стабильной топологии обслуживания, а затем возобновили обработку заданий в очереди.

#### Таймлайн: 

- **2018 21 октября 22:52 UTC** во время разделения сети Orchestrator, который был активен в нашем основном центре 
обработки данных, начал процесс отмены выбора лидера и назначение нового. После восстановления связи трафик был 
направлен на новые основные узлы. В итоге кластеры баз данных в обоих центрах обработки данных теперь содержали 
операции записи, которых не было в другом центре обработки данных, и безопасно перенести основной сервер в 
восточный центр обработки данных не получилось
- **2018 21 октября 22:54 UTC** внутренние системы мониторинга начали генерировать оповещения, указывающие на 
многочисленные сбои в наших системах.
- **2018 21 октября 23:02 UTC** проблема была локализована
- **2018 21 октября 23:07 UTC** группа реагирования решила вручную заблокировать наши внутренние инструменты 
развертывания, чтобы предотвратить внесение каких-либо дополнительных изменений.
- **2018 21 октября 23:09 UTC** группа реагирования перевела сайт в желтый статус. Это действие автоматически 
перевело ситуацию в активный инцидент и отправило предупреждение координатору инцидента. 
- **2018 21 октября 23:11 UTC** подключился координатор инцидентов, и через две минуты изменил статус решения на красный.
- **2018 21 октября 23:13 UTC** Были вызваны дополнительные инженеры из группы разработки баз данных GitHub. Они 
начали исследовать текущее состояние, чтобы определить, какие действия необходимо предпринять, чтобы вручную настроить 
базу данных Восточного побережья США в качестве основной для каждого кластера и перестроить топологию репликации. 
Эта попытка была сложной, поскольку к этому моменту кластер базы данных Западного побережья принимал записи с нашего 
уровня приложений в течение почти 40 минут. Кроме того, в кластере Восточного побережья существовало несколько секунд 
записей, которые не были реплицированы на Западное побережье и препятствовали репликации новых записей обратно на 
Восточное побережье 
- **2018 21 октября 23:19 UTC** После запроса, состояния кластеров базы данных стало ясно, что нам нужно остановить 
выполнение заданий, записывающих метаданные о таких вещах, как push-уведомления. Мы сделали явный выбор частично 
снизить удобство использования сайта, приостановив доставку веб-перехватчиков и сборку страниц GitHub вместо того, 
чтобы поставить под угрозу данные, которые мы уже получили от пользователей
- **2018 22 октября 00:05 UTC** Инженеры, участвующие в группе реагирования на инциденты, начали разработку плана 
по устранению несоответствий данных и реализации наших процедур аварийного переключения для MySQL. Был обновлен 
статус, чтобы сообщить пользователям, что мы собираемся выполнить контролируемый переход на другой ресурс внутренней 
системы хранения данных.
- **2018 22 октября 00:41 UTC** начат процесс резервного копирования для всех затронутых кластеров MySQL
- **2018 22 октября 06:51 UTC** Несколько кластеров завершили восстановление из резервных копий в нашем центре 
обработки данных на восточном побережье США и начали репликацию новых данных с западного побережья. 
- **2018 22 октября 07:46 UTC** GitHub опубликовал сообщение в блоге , чтобы предоставить больше контекста.
- **2018 22 октября 11:12 UTC** Все первичные базы данных снова установлены на восточном побережье США. В результате 
сайт стал гораздо более отзывчивым, поскольку записи теперь направлялись на сервер базы данных, который находился в 
том же физическом центре обработки данных, что и наш уровень приложений. Несмотря на существенное повышение 
производительности, десятки реплик чтения базы данных по-прежнему отставали от основной на несколько часов.
- **2018 22 октября 16:24 UTC** переход на исходную топологию, решая непосредственные проблемы с задержкой и доступностью.
- **2018 22 октября 16:45 UTC** Возобновление обработки заданий в очереди 
- **2018 22 октября 23:03 UTC** Все ожидающие сборки веб-перехватчиков и страниц были обработаны, а целостность и 
правильная работа всех систем подтверждены. Статус сайта изменился на зеленый.

#### Последующие действия

1. Настройка конфигурации Orchestrator, чтобы предотвратить распространение основных баз данных за пределы 
региональных границ.
2. Переход на новый механизм отчетности о статусе, который предоставит нам более обширную площадку для обсуждения 
активных инцидентов более четким и понятным языком
3. Поддержка обслуживания трафика GitHub из нескольких центров обработки данных в схеме «активный-активный-активный».

</details>